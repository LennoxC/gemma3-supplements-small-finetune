{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h77TYIyDvSYg8rYt9OA1EH9EmowYMknu",
      "authorship_tag": "ABX9TyP7w89+EmKLajUcv2yjTLNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennoxC/gemma3-supplements-small-finetune/blob/main/gemma3_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma3-4b fine-tune for food and beverage label OCR-VQA"
      ],
      "metadata": {
        "id": "C4SwckN0bDGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElA7flCNZbtK"
      },
      "outputs": [],
      "source": [
        "#%pip install torch tensorboard\n",
        "#%pip install transformers datasets accelerate evaluate trl protobuf sentencepiece\n",
        "#%pip install flash-attn # only if GPU supports flashAttention (nvidia Ampere)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "viG6zFmPbjyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "duB2IDCaeeOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "xh2WMSV4bi-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for saving results on google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "58429SuNb4pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"google/gemma-3-4b-it\"\n",
        "checkpoint_dir = \"/content/drive/MyDrive/MyGemmaNPC\"\n",
        "learning_rate = 5e-5"
      ],
      "metadata": {
        "id": "awDbCORgcbg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a quality control robot responsible for monitoring the quality of supplement labels.\"\n",
        "user_prompt = \"\"\"Using primarily the text contained in the attached label supplement image, answer the list of questions in the <QUESTIONS> tags.\n",
        "Answer concisely in a JSON format with no preamble, allowing the response to easily be parsed. An example response would be:\n",
        "{\n",
        "  \"brand\": \"label supplelments co\",\n",
        "  \"contents\": 120\n",
        "}\n",
        "\n",
        "<QUESTIONS>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6pIe6Tfzt_wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": sample[\"questions\"]\n",
        "                        #\"text\": user_prompt.format(\n",
        "                        #    questions=sample[\"questions\"]\n",
        "                        #),\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                        \"image\": sample[\"image\"],\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": sample[\"answers\"]}],\n",
        "            },\n",
        "        ],\n",
        "    }"
      ],
      "metadata": {
        "id": "WgClnPnSt1cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class OCRVQADataset(Dataset):\n",
        "    def __init__(self, jsonl_file, transform=None, min_q=1, max_q=4):\n",
        "        with open(jsonl_file, 'r') as f:\n",
        "            self.samples = [json.loads(line) for line in f]\n",
        "        self.transform = transform or transforms.ToTensor()\n",
        "        self.min_q = min_q\n",
        "        self.max_q = max_q\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Keep as PIL so process_vision_info works later\n",
        "        image_path = \"/content/drive/MyDrive/datasets/small-supplements-ocrvqa/images/\" + sample[\"image\"]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        qa_pairs = sample[\"qas\"]\n",
        "        k = random.randint(self.min_q, min(self.max_q, len(qa_pairs)))\n",
        "        chosen_pairs = random.sample(qa_pairs, k)\n",
        "\n",
        "        questions_str = (\n",
        "            user_prompt\n",
        "            + \"; \".join(f\"Question: {p['q']} This corresponds to JSON key {p['k']}\" for p in chosen_pairs)\n",
        "            + \"</QUESTIONS>\"\n",
        "        )\n",
        "        answers_dict = {p['k']: p['a'] for p in chosen_pairs}\n",
        "        answers_str = json.dumps(answers_dict, ensure_ascii=False)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,  # PIL\n",
        "            \"questions\": questions_str,\n",
        "            \"answers\": answers_str\n",
        "        }"
      ],
      "metadata": {
        "id": "YT0xw4KSgZ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
        "    image_inputs = []\n",
        "    for msg in messages:\n",
        "        content = msg.get(\"content\", [])\n",
        "        if not isinstance(content, list):\n",
        "            content = [content]\n",
        "\n",
        "        for element in content:\n",
        "            if isinstance(element, dict) and (\n",
        "                \"image\" in element or element.get(\"type\") == \"image\"\n",
        "            ):\n",
        "                img = element.get(\"image\", element)\n",
        "                if isinstance(img, str):\n",
        "                    img = Image.open(img).convert(\"RGB\")\n",
        "                elif not isinstance(img, Image.Image):\n",
        "                    raise ValueError(f\"Unsupported image type: {type(img)}\")\n",
        "                image_inputs.append(img)\n",
        "    return image_inputs\n"
      ],
      "metadata": {
        "id": "kISLJ2tduRvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/datasets/small-supplements-ocrvqa/output.jsonl\"\n",
        "dataset_obj = OCRVQADataset(dataset_path)"
      ],
      "metadata": {
        "id": "OMrFZ6siIKCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set split sizes\n",
        "total_size = len(dataset_obj)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Use a generator with a manual seed for reproducibility\n",
        "generator = torch.Generator().manual_seed(42)"
      ],
      "metadata": {
        "id": "MqKFo5thF8_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset_obj, [train_size, val_size, test_size], generator=generator\n",
        ")\n",
        "\n",
        "train_dataset_fmt = [format_data(sample) for sample in train_dataset]\n",
        "\n",
        "print(train_dataset_fmt[100])"
      ],
      "metadata": {
        "id": "iKvgPfNGiSzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-4b-it\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n",
        "\n",
        "# Check if GPU benefits from bfloat16\n",
        "#if torch.cuda.get_device_capability()[0] < 8:\n",
        "#    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "# Define model init arguments\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
        "    torch_dtype=torch.float16, # What torch dtype to use, defaults to auto\n",
        "    device_map=\"auto\", # Let torch decide how to load the model\n",
        ")\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "#model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "#    load_in_4bit=True,\n",
        "#    bnb_4bit_use_double_quant=True,\n",
        "#    bnb_4bit_quant_type=\"nf4\",\n",
        "#    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "#    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        "#)\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(base_model)"
      ],
      "metadata": {
        "id": "yraPRRaI5oLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install peft"
      ],
      "metadata": {
        "id": "CoFyIeFVKVdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "v3KB38Mj56R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"gemma-supplements-small\",       # directory to save and repository id\n",
        "    num_train_epochs=1,                         # number of training epochs\n",
        "    per_device_train_batch_size=1,              # batch size per device during training\n",
        "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
        "    logging_steps=5,                            # log every 5 steps\n",
        "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
        "    bf16=True,                                  # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
        "    push_to_hub=True,                           # push model to hub\n",
        "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False\n",
        "    },  # use reentrant checkpointing\n",
        "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
        ")\n",
        "args.remove_unused_columns = False # important for collator\n",
        "\n",
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        image_inputs = process_vision_info(example[\"messages\"])\n",
        "        text = processor.apply_chat_template(\n",
        "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "        )\n",
        "        texts.append(text.strip())\n",
        "        images.append(image_inputs)\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens for not being used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "UAObWZGX568Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset_fmt,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "v02de0fu6P1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training, the model will be automatically saved to the Hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model again to the Hugging Face Hub\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "y9b_q6xU6STw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}